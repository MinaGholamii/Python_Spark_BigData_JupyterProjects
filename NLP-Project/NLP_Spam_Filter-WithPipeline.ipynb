{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93de22df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "| _c0|                 _c1|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "|spam|Free entry in 2 a...|\n",
      "| ham|U dun say so earl...|\n",
      "| ham|Nah I don't think...|\n",
      "|spam|FreeMsg Hey there...|\n",
      "| ham|Even my brother i...|\n",
      "| ham|As per your reque...|\n",
      "|spam|WINNER!! As a val...|\n",
      "|spam|Had your mobile 1...|\n",
      "| ham|I'm gonna be home...|\n",
      "|spam|SIX chances to wi...|\n",
      "|spam|URGENT! You have ...|\n",
      "| ham|I've been searchi...|\n",
      "| ham|I HAVE A DATE ON ...|\n",
      "|spam|XXXMobileMovieClu...|\n",
      "| ham|Oh k...i'm watchi...|\n",
      "| ham|Eh u remember how...|\n",
      "| ham|Fine if thats th...|\n",
      "|spam|England v Macedon...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the findspark library to locate and initialize Spark\n",
    "import findspark\n",
    "\n",
    "# Initialize Spark with the path to your Spark installation\n",
    "findspark.init('/home/mina/python-spark/spark-3.4.0-bin-hadoop3/')\n",
    "\n",
    "# Import the pyspark library\n",
    "import pyspark\n",
    "\n",
    "# Import the SparkSession class from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession with the specified application name\n",
    "spark = SparkSession.builder.appName('NLP_Project').getOrCreate()\n",
    "\n",
    "# Read a CSV file into a DataFrame named 'data' with schema inference and '\\t' as the separator\n",
    "data = spark.read.csv('smsspamcollection/SMSSpamCollection', inferSchema = True , sep ='\\t')\n",
    "\n",
    "# Show the contents of the DataFrame 'data'\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef5eb11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                text|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "| spam|FreeMsg Hey there...|\n",
      "|  ham|Even my brother i...|\n",
      "|  ham|As per your reque...|\n",
      "| spam|WINNER!! As a val...|\n",
      "| spam|Had your mobile 1...|\n",
      "|  ham|I'm gonna be home...|\n",
      "| spam|SIX chances to wi...|\n",
      "| spam|URGENT! You have ...|\n",
      "|  ham|I've been searchi...|\n",
      "|  ham|I HAVE A DATE ON ...|\n",
      "| spam|XXXMobileMovieClu...|\n",
      "|  ham|Oh k...i'm watchi...|\n",
      "|  ham|Eh u remember how...|\n",
      "|  ham|Fine if thats th...|\n",
      "| spam|England v Macedon...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename the columns '_c0' to 'class' and '_c1' to 'text' in the DataFrame 'data'\n",
    "data = data.withColumnRenamed('_c0','class').withColumnRenamed('_c1','text')\n",
    "\n",
    "# Show the contents of the DataFrame 'data' after renaming the columns\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ecc3b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|                text|lenght|\n",
      "+-----+--------------------+------+\n",
      "|  ham|Go until jurong p...|   111|\n",
      "|  ham|Ok lar... Joking ...|    29|\n",
      "| spam|Free entry in 2 a...|   155|\n",
      "|  ham|U dun say so earl...|    49|\n",
      "|  ham|Nah I don't think...|    61|\n",
      "| spam|FreeMsg Hey there...|   147|\n",
      "|  ham|Even my brother i...|    77|\n",
      "|  ham|As per your reque...|   160|\n",
      "| spam|WINNER!! As a val...|   157|\n",
      "| spam|Had your mobile 1...|   154|\n",
      "|  ham|I'm gonna be home...|   109|\n",
      "| spam|SIX chances to wi...|   136|\n",
      "| spam|URGENT! You have ...|   155|\n",
      "|  ham|I've been searchi...|   196|\n",
      "|  ham|I HAVE A DATE ON ...|    35|\n",
      "| spam|XXXMobileMovieClu...|   149|\n",
      "|  ham|Oh k...i'm watchi...|    26|\n",
      "|  ham|Eh u remember how...|    81|\n",
      "|  ham|Fine if thats th...|    56|\n",
      "| spam|England v Macedon...|   155|\n",
      "+-----+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the length function from pyspark.sql.functions\n",
    "from pyspark.sql.functions import length\n",
    "\n",
    "# Create a new DataFrame 'new_data' with an additional 'length' column\n",
    "new_data = data.withColumn('lenght' , length(data['text']))\n",
    "\n",
    "# Show the contents of the DataFrame 'new_data' with the added 'length' column\n",
    "new_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "028c1ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|class|      avg(lenght)|\n",
      "+-----+-----------------+\n",
      "|  ham|71.45431945307645|\n",
      "| spam|138.6706827309237|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group the 'new_data' DataFrame by the 'class' column and calculate the mean of the 'length' column for each group\n",
    "mean_class = new_data.groupBy('class').mean('lenght')\n",
    "\n",
    "# Show the mean length for each class\n",
    "mean_class.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e02f5dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary classes from pyspark.ml.feature\n",
    "from pyspark.ml.feature import (Tokenizer,StopWordsRemover,CountVectorizer,IDF,StringIndexer)\n",
    "\n",
    "# Create a Tokenizer instance with input column 'text' and output column 'words'\n",
    "tokenizer = Tokenizer(inputCol='text' , outputCol='words')\n",
    "\n",
    "# Create a StopWordsRemover instance with input column 'words' and output column 'stopword'\n",
    "stopword = StopWordsRemover(inputCol='words' , outputCol='stopword')\n",
    "\n",
    "# Create a CountVectorizer instance with input column 'stopword' and output column 'cv_word'\n",
    "cv = CountVectorizer(inputCol='stopword' , outputCol='cv_word')\n",
    "\n",
    "# Create an IDF instance with input column 'cv_word' and output column 'idf'\n",
    "idf = IDF(inputCol='cv_word' , outputCol='idf')\n",
    "\n",
    "# Create a StringIndexer instance with input column 'class' and output column 'label'\n",
    "ham_spam_convert = StringIndexer(inputCol='class', outputCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4aab5378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the VectorAssembler class from pyspark.ml.feature\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create a VectorAssembler instance with specified input columns 'idf' and 'length', and output column 'features'\n",
    "assembler = VectorAssembler(inputCols=['idf', 'lenght'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4968a5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "|class|                text|lenght|               words|            stopword|             cv_word|                 idf|label|            features|\n",
      "+-----+--------------------+------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "|  ham|Go until jurong p...|   111|[go, until, juron...|[go, jurong, poin...|(13423,[7,11,31,6...|(13423,[7,11,31,6...|  0.0|(13424,[7,11,31,6...|\n",
      "|  ham|Ok lar... Joking ...|    29|[ok, lar..., joki...|[ok, lar..., joki...|(13423,[0,24,301,...|(13423,[0,24,301,...|  0.0|(13424,[0,24,301,...|\n",
      "| spam|Free entry in 2 a...|   155|[free, entry, in,...|[free, entry, 2, ...|(13423,[2,13,19,3...|(13423,[2,13,19,3...|  1.0|(13424,[2,13,19,3...|\n",
      "|  ham|U dun say so earl...|    49|[u, dun, say, so,...|[u, dun, say, ear...|(13423,[0,70,80,1...|(13423,[0,70,80,1...|  0.0|(13424,[0,70,80,1...|\n",
      "|  ham|Nah I don't think...|    61|[nah, i, don't, t...|[nah, think, goes...|(13423,[36,134,31...|(13423,[36,134,31...|  0.0|(13424,[36,134,31...|\n",
      "| spam|FreeMsg Hey there...|   147|[freemsg, hey, th...|[freemsg, hey, da...|(13423,[10,60,140...|(13423,[10,60,140...|  1.0|(13424,[10,60,140...|\n",
      "|  ham|Even my brother i...|    77|[even, my, brothe...|[even, brother, l...|(13423,[10,53,102...|(13423,[10,53,102...|  0.0|(13424,[10,53,102...|\n",
      "|  ham|As per your reque...|   160|[as, per, your, r...|[per, request, 'm...|(13423,[127,185,4...|(13423,[127,185,4...|  0.0|(13424,[127,185,4...|\n",
      "| spam|WINNER!! As a val...|   157|[winner!!, as, a,...|[winner!!, valued...|(13423,[1,47,121,...|(13423,[1,47,121,...|  1.0|(13424,[1,47,121,...|\n",
      "| spam|Had your mobile 1...|   154|[had, your, mobil...|[mobile, 11, mont...|(13423,[0,1,13,27...|(13423,[0,1,13,27...|  1.0|(13424,[0,1,13,27...|\n",
      "|  ham|I'm gonna be home...|   109|[i'm, gonna, be, ...|[gonna, home, soo...|(13423,[18,43,117...|(13423,[18,43,117...|  0.0|(13424,[18,43,117...|\n",
      "| spam|SIX chances to wi...|   136|[six, chances, to...|[six, chances, wi...|(13423,[8,16,37,8...|(13423,[8,16,37,8...|  1.0|(13424,[8,16,37,8...|\n",
      "| spam|URGENT! You have ...|   155|[urgent!, you, ha...|[urgent!, won, 1,...|(13423,[13,30,47,...|(13423,[13,30,47,...|  1.0|(13424,[13,30,47,...|\n",
      "|  ham|I've been searchi...|   196|[i've, been, sear...|[searching, right...|(13423,[39,95,221...|(13423,[39,95,221...|  0.0|(13424,[39,95,221...|\n",
      "|  ham|I HAVE A DATE ON ...|    35|[i, have, a, date...|[date, sunday, wi...|(13423,[555,1797,...|(13423,[555,1797,...|  0.0|(13424,[555,1797,...|\n",
      "| spam|XXXMobileMovieClu...|   149|[xxxmobilemoviecl...|[xxxmobilemoviecl...|(13423,[30,109,11...|(13423,[30,109,11...|  1.0|(13424,[30,109,11...|\n",
      "|  ham|Oh k...i'm watchi...|    26|[oh, k...i'm, wat...|[oh, k...i'm, wat...|(13423,[82,214,44...|(13423,[82,214,44...|  0.0|(13424,[82,214,44...|\n",
      "|  ham|Eh u remember how...|    81|[eh, u, remember,...|[eh, u, remember,...|(13423,[0,2,49,13...|(13423,[0,2,49,13...|  0.0|(13424,[0,2,49,13...|\n",
      "|  ham|Fine if thats th...|    56|[fine, if, thats...|[fine, thats, wa...|(13423,[0,74,105,...|(13423,[0,74,105,...|  0.0|(13424,[0,74,105,...|\n",
      "| spam|England v Macedon...|   155|[england, v, mace...|[england, v, mace...|(13423,[4,30,33,5...|(13423,[4,30,33,5...|  1.0|(13424,[4,30,33,5...|\n",
      "+-----+--------------------+------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the Pipeline class from pyspark.ml\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Create a Pipeline with a list of stages representing the data transformation steps\n",
    "pipe = Pipeline(stages=[tokenizer, stopword, cv, idf,ham_spam_convert,assembler])\n",
    "\n",
    "# Fit the pipeline on the 'new_data' DataFrame, which applies each stage's transformation to the data\n",
    "data_pipe_fit = pipe.fit(new_data)\n",
    "\n",
    "# Transform the 'new_data' DataFrame using the fitted pipeline to produce 'data_pipe_tranc'\n",
    "data_pipe_tranc = data_pipe_fit.transform(new_data)\n",
    "\n",
    "# Show the transformed DataFrame 'data_pipe_tranc'\n",
    "data_pipe_tranc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a286a888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class',\n",
       " 'text',\n",
       " 'lenght',\n",
       " 'words',\n",
       " 'stopword',\n",
       " 'cv_word',\n",
       " 'idf',\n",
       " 'label',\n",
       " 'features']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the column names of the 'data_pipe_tranc' DataFrame\n",
    "data_pipe_tranc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "805bf5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(13424,[7,11,31,6...|  0.0|\n",
      "|(13424,[0,24,301,...|  0.0|\n",
      "|(13424,[2,13,19,3...|  1.0|\n",
      "|(13424,[0,70,80,1...|  0.0|\n",
      "|(13424,[36,134,31...|  0.0|\n",
      "|(13424,[10,60,140...|  1.0|\n",
      "|(13424,[10,53,102...|  0.0|\n",
      "|(13424,[127,185,4...|  0.0|\n",
      "|(13424,[1,47,121,...|  1.0|\n",
      "|(13424,[0,1,13,27...|  1.0|\n",
      "|(13424,[18,43,117...|  0.0|\n",
      "|(13424,[8,16,37,8...|  1.0|\n",
      "|(13424,[13,30,47,...|  1.0|\n",
      "|(13424,[39,95,221...|  0.0|\n",
      "|(13424,[555,1797,...|  0.0|\n",
      "|(13424,[30,109,11...|  1.0|\n",
      "|(13424,[82,214,44...|  0.0|\n",
      "|(13424,[0,2,49,13...|  0.0|\n",
      "|(13424,[0,74,105,...|  0.0|\n",
      "|(13424,[4,30,33,5...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the 'features' and 'label' columns from the 'data_pipe_tranc' DataFrame and create a new DataFrame 'final_data'\n",
    "final_data = data_pipe_tranc.select('features','label')\n",
    "\n",
    "# Show the contents of the 'final_data' DataFrame\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e276483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/19 09:58:39 WARN DAGScheduler: Broadcasting large task binary with size 1131.3 KiB\n",
      "23/09/19 09:58:40 WARN DAGScheduler: Broadcasting large task binary with size 1105.2 KiB\n",
      "23/09/19 09:58:40 WARN DAGScheduler: Broadcasting large task binary with size 1336.7 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(13424,[0,1,2,12,...|  1.0|[-1144.2264606083...|[1.46430636370738...|       1.0|\n",
      "|(13424,[0,1,2,12,...|  1.0|[-1142.1225033729...|[1.43771372374487...|       1.0|\n",
      "|(13424,[0,1,2,13,...|  0.0|[-608.43874039574...|[1.0,6.2032321227...|       0.0|\n",
      "|(13424,[0,1,2,15,...|  1.0|[-1154.1128942288...|[6.03435503490663...|       1.0|\n",
      "|(13424,[0,1,4,13,...|  1.0|[-1428.2838348287...|[4.76835746901457...|       1.0|\n",
      "|(13424,[0,1,4,50,...|  0.0|[-827.19824673355...|[1.0,2.1485954144...|       0.0|\n",
      "|(13424,[0,1,4,137...|  1.0|[-1792.3271693142...|[1.68951245805226...|       1.0|\n",
      "|(13424,[0,1,7,8,1...|  0.0|[-1175.2086573754...|[1.0,9.7601529288...|       0.0|\n",
      "|(13424,[0,1,7,15,...|  0.0|[-664.19069718438...|[1.0,2.2988465945...|       0.0|\n",
      "|(13424,[0,1,9,14,...|  0.0|[-538.81334369834...|[1.0,6.4314525443...|       0.0|\n",
      "|(13424,[0,1,10,20...|  1.0|[-1060.5058926077...|[1.38088828705487...|       1.0|\n",
      "|(13424,[0,1,12,33...|  0.0|[-448.11819571767...|[1.0,7.8982782087...|       0.0|\n",
      "|(13424,[0,1,13,27...|  1.0|[-1186.7089228719...|[1.35049733769203...|       1.0|\n",
      "|(13424,[0,1,13,27...|  1.0|[-1186.7089228719...|[1.35049733769203...|       1.0|\n",
      "|(13424,[0,1,14,31...|  0.0|[-218.48298483858...|[1.0,9.9782220965...|       0.0|\n",
      "|(13424,[0,1,20,27...|  0.0|[-974.35314270343...|[0.99999999999981...|       0.0|\n",
      "|(13424,[0,1,20,47...|  1.0|[-1197.1986205748...|[4.12958552933831...|       1.0|\n",
      "|(13424,[0,1,21,27...|  0.0|[-749.01775956245...|[1.0,2.3029650823...|       0.0|\n",
      "|(13424,[0,1,23,63...|  0.0|[-1311.7980599492...|[1.0,1.7269366719...|       0.0|\n",
      "|(13424,[0,1,27,35...|  0.0|[-1470.6551287029...|[0.99999999999994...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the NaiveBayes class from pyspark.ml.classification\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "# Create a NaiveBayes model instance\n",
    "model_1 = NaiveBayes()\n",
    "\n",
    "# Split the 'final_data' DataFrame into training and testing sets with a 70-30 ratio\n",
    "train, test = final_data.randomSplit([0.7,0.3])\n",
    "\n",
    "# Fit the NaiveBayes model on the training data\n",
    "model_fit_1 = model_1.fit(train)\n",
    "\n",
    "# Transform the test data using the fitted model to make predictions\n",
    "model_test_1 = model_fit_1.transform(test)\n",
    "\n",
    "# Show the results, including predictions, in the 'model_test_1' DataFrame\n",
    "model_test_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04ea8e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/19 09:58:40 WARN DAGScheduler: Broadcasting large task binary with size 1341.3 KiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9238406554058418"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the MulticlassClassificationEvaluator class from pyspark.ml.evaluation\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Create an instance of the MulticlassClassificationEvaluator\n",
    "eval_model_1 = MulticlassClassificationEvaluator()\n",
    "\n",
    "# Evaluate the model's performance on the test data and calculate the accuracy\n",
    "acc_1 = eval_model_1.evaluate(model_test_1)\n",
    "\n",
    "# Display the accuracy of the model\n",
    "acc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00835637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/19 09:58:41 WARN DAGScheduler: Broadcasting large task binary with size 1128.2 KiB\n",
      "23/09/19 09:58:41 WARN DAGScheduler: Broadcasting large task binary with size 1128.2 KiB\n",
      "23/09/19 09:58:41 WARN DAGScheduler: Broadcasting large task binary with size 1267.1 KiB\n",
      "23/09/19 09:58:44 WARN DAGScheduler: Broadcasting large task binary with size 1494.1 KiB\n",
      "23/09/19 09:58:45 WARN DAGScheduler: Broadcasting large task binary with size 1541.4 KiB\n",
      "23/09/19 09:58:45 WARN DAGScheduler: Broadcasting large task binary with size 1588.3 KiB\n",
      "23/09/19 09:58:45 WARN DAGScheduler: Broadcasting large task binary with size 1634.4 KiB\n",
      "23/09/19 09:58:46 WARN DAGScheduler: Broadcasting large task binary with size 1682.7 KiB\n",
      "23/09/19 09:58:46 WARN DAGScheduler: Broadcasting large task binary with size 1430.4 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(13424,[0,1,2,12,...|  1.0|[73.9955868850982...|[0.73995586885098...|       0.0|\n",
      "|(13424,[0,1,2,13,...|  0.0|[82.1020981056429...|[0.82102098105642...|       0.0|\n",
      "|(13424,[0,1,2,15,...|  1.0|[67.9045675664677...|[0.67904567566467...|       0.0|\n",
      "|(13424,[0,1,2,15,...|  1.0|[69.5083779286195...|[0.69508377928619...|       0.0|\n",
      "|(13424,[0,1,2,41,...|  0.0|[83.2912386785097...|[0.83291238678509...|       0.0|\n",
      "|(13424,[0,1,4,137...|  1.0|[81.1750539098457...|[0.81175053909845...|       0.0|\n",
      "|(13424,[0,1,5,20,...|  0.0|[86.2891199955098...|[0.86289119995509...|       0.0|\n",
      "|(13424,[0,1,7,8,1...|  0.0|[83.0186019698717...|[0.83018601969871...|       0.0|\n",
      "|(13424,[0,1,9,14,...|  0.0|[86.6973852356783...|[0.86697385235678...|       0.0|\n",
      "|(13424,[0,1,13,18...|  1.0|[70.5821487659635...|[0.70582148765963...|       0.0|\n",
      "|(13424,[0,1,14,18...|  0.0|[86.2623600784863...|[0.86262360078486...|       0.0|\n",
      "|(13424,[0,1,14,31...|  0.0|[86.6973852356783...|[0.86697385235678...|       0.0|\n",
      "|(13424,[0,1,16,19...|  0.0|[83.7438799131265...|[0.83743879913126...|       0.0|\n",
      "|(13424,[0,1,21,27...|  0.0|[86.6973852356783...|[0.86697385235678...|       0.0|\n",
      "|(13424,[0,1,23,63...|  0.0|[86.2623600784863...|[0.86262360078486...|       0.0|\n",
      "|(13424,[0,1,27,85...|  0.0|[86.2623600784863...|[0.86262360078486...|       0.0|\n",
      "|(13424,[0,1,38,98...|  1.0|[70.7548208321672...|[0.70754820832167...|       0.0|\n",
      "|(13424,[0,1,47,91...|  1.0|[60.5673444236913...|[0.60567344423691...|       0.0|\n",
      "|(13424,[0,1,98,18...|  1.0|[70.9162027715552...|[0.70916202771555...|       0.0|\n",
      "|(13424,[0,1,428,6...|  0.0|[86.6973852356783...|[0.86697385235678...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the RandomForestClassifier class from pyspark.ml.classification\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Create a RandomForestClassifier model instance with 100 trees\n",
    "model_2 = RandomForestClassifier(numTrees=100)\n",
    "\n",
    "# Split the 'final_data' DataFrame into training and testing sets with a 70-30 ratio\n",
    "train, test = final_data.randomSplit([0.7,0.3])\n",
    "\n",
    "# Fit the RandomForestClassifier model on the training data\n",
    "model_fit_2 = model_2.fit(train)\n",
    "\n",
    "# Transform the test data using the fitted model to make predictions\n",
    "model_test_2 = model_fit_2.transform(test)\n",
    "\n",
    "# Show the results, including predictions, in the 'model_test_2' DataFrame\n",
    "model_test_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c1b5b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/19 09:58:47 WARN DAGScheduler: Broadcasting large task binary with size 1435.0 KiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8048182251628536"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the MulticlassClassificationEvaluator class from pyspark.ml.evaluation\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Create an instance of the MulticlassClassificationEvaluator\n",
    "eval_model_2 = MulticlassClassificationEvaluator()\n",
    "\n",
    "# Evaluate the performance of the model_2 (Random Forest) on the test data and calculate the accuracy\n",
    "acc_2 = eval_model_2.evaluate(model_test_2)\n",
    "\n",
    "# Display the accuracy of the model\n",
    "acc_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54d1e391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/19 09:58:47 WARN DAGScheduler: Broadcasting large task binary with size 1127.7 KiB\n",
      "23/09/19 09:58:47 WARN DAGScheduler: Broadcasting large task binary with size 1127.8 KiB\n",
      "23/09/19 09:58:48 WARN DAGScheduler: Broadcasting large task binary with size 1266.7 KiB\n",
      "23/09/19 09:58:50 WARN DAGScheduler: Broadcasting large task binary with size 1443.5 KiB\n",
      "23/09/19 09:58:51 WARN DAGScheduler: Broadcasting large task binary with size 1444.3 KiB\n",
      "23/09/19 09:58:51 WARN DAGScheduler: Broadcasting large task binary with size 1444.8 KiB\n",
      "23/09/19 09:58:51 WARN DAGScheduler: Broadcasting large task binary with size 1445.9 KiB\n",
      "23/09/19 09:58:51 WARN DAGScheduler: Broadcasting large task binary with size 1447.5 KiB\n",
      "23/09/19 09:58:52 WARN DAGScheduler: Broadcasting large task binary with size 1452.7 KiB\n",
      "23/09/19 09:58:52 WARN DAGScheduler: Broadcasting large task binary with size 1453.2 KiB\n",
      "23/09/19 09:58:52 WARN DAGScheduler: Broadcasting large task binary with size 1453.7 KiB\n",
      "23/09/19 09:58:53 WARN DAGScheduler: Broadcasting large task binary with size 1454.9 KiB\n",
      "23/09/19 09:58:53 WARN DAGScheduler: Broadcasting large task binary with size 1456.8 KiB\n",
      "23/09/19 09:58:53 WARN DAGScheduler: Broadcasting large task binary with size 1458.2 KiB\n",
      "23/09/19 09:58:53 WARN DAGScheduler: Broadcasting large task binary with size 1458.7 KiB\n",
      "23/09/19 09:58:54 WARN DAGScheduler: Broadcasting large task binary with size 1459.3 KiB\n",
      "23/09/19 09:58:54 WARN DAGScheduler: Broadcasting large task binary with size 1460.4 KiB\n",
      "23/09/19 09:58:54 WARN DAGScheduler: Broadcasting large task binary with size 1462.3 KiB\n",
      "23/09/19 09:58:55 WARN DAGScheduler: Broadcasting large task binary with size 1463.7 KiB\n",
      "23/09/19 09:58:55 WARN DAGScheduler: Broadcasting large task binary with size 1464.2 KiB\n",
      "23/09/19 09:58:55 WARN DAGScheduler: Broadcasting large task binary with size 1464.8 KiB\n",
      "23/09/19 09:58:55 WARN DAGScheduler: Broadcasting large task binary with size 1465.9 KiB\n",
      "23/09/19 09:58:56 WARN DAGScheduler: Broadcasting large task binary with size 1468.1 KiB\n",
      "23/09/19 09:58:56 WARN DAGScheduler: Broadcasting large task binary with size 1469.6 KiB\n",
      "23/09/19 09:58:56 WARN DAGScheduler: Broadcasting large task binary with size 1470.0 KiB\n",
      "23/09/19 09:58:57 WARN DAGScheduler: Broadcasting large task binary with size 1470.6 KiB\n",
      "23/09/19 09:58:57 WARN DAGScheduler: Broadcasting large task binary with size 1471.7 KiB\n",
      "23/09/19 09:58:57 WARN DAGScheduler: Broadcasting large task binary with size 1473.7 KiB\n",
      "23/09/19 09:58:58 WARN DAGScheduler: Broadcasting large task binary with size 1475.7 KiB\n",
      "23/09/19 09:58:58 WARN DAGScheduler: Broadcasting large task binary with size 1476.2 KiB\n",
      "23/09/19 09:58:58 WARN DAGScheduler: Broadcasting large task binary with size 1476.8 KiB\n",
      "23/09/19 09:58:58 WARN DAGScheduler: Broadcasting large task binary with size 1477.9 KiB\n",
      "23/09/19 09:58:59 WARN DAGScheduler: Broadcasting large task binary with size 1479.3 KiB\n",
      "23/09/19 09:58:59 WARN DAGScheduler: Broadcasting large task binary with size 1480.7 KiB\n",
      "23/09/19 09:58:59 WARN DAGScheduler: Broadcasting large task binary with size 1481.2 KiB\n",
      "23/09/19 09:59:00 WARN DAGScheduler: Broadcasting large task binary with size 1481.8 KiB\n",
      "23/09/19 09:59:00 WARN DAGScheduler: Broadcasting large task binary with size 1482.9 KiB\n",
      "23/09/19 09:59:00 WARN DAGScheduler: Broadcasting large task binary with size 1484.8 KiB\n",
      "23/09/19 09:59:01 WARN DAGScheduler: Broadcasting large task binary with size 1486.6 KiB\n",
      "23/09/19 09:59:01 WARN DAGScheduler: Broadcasting large task binary with size 1487.0 KiB\n",
      "23/09/19 09:59:01 WARN DAGScheduler: Broadcasting large task binary with size 1487.6 KiB\n",
      "23/09/19 09:59:01 WARN DAGScheduler: Broadcasting large task binary with size 1488.7 KiB\n",
      "23/09/19 09:59:02 WARN DAGScheduler: Broadcasting large task binary with size 1490.4 KiB\n",
      "23/09/19 09:59:02 WARN DAGScheduler: Broadcasting large task binary with size 1491.7 KiB\n",
      "23/09/19 09:59:02 WARN DAGScheduler: Broadcasting large task binary with size 1492.2 KiB\n",
      "23/09/19 09:59:03 WARN DAGScheduler: Broadcasting large task binary with size 1492.8 KiB\n",
      "23/09/19 09:59:03 WARN DAGScheduler: Broadcasting large task binary with size 1493.9 KiB\n",
      "23/09/19 09:59:03 WARN DAGScheduler: Broadcasting large task binary with size 1495.5 KiB\n",
      "23/09/19 09:59:04 WARN DAGScheduler: Broadcasting large task binary with size 1496.9 KiB\n",
      "23/09/19 09:59:04 WARN DAGScheduler: Broadcasting large task binary with size 1497.4 KiB\n",
      "23/09/19 09:59:04 WARN DAGScheduler: Broadcasting large task binary with size 1498.0 KiB\n",
      "23/09/19 09:59:04 WARN DAGScheduler: Broadcasting large task binary with size 1498.9 KiB\n",
      "23/09/19 09:59:05 WARN DAGScheduler: Broadcasting large task binary with size 1500.3 KiB\n",
      "23/09/19 09:59:05 WARN DAGScheduler: Broadcasting large task binary with size 1502.0 KiB\n",
      "23/09/19 09:59:05 WARN DAGScheduler: Broadcasting large task binary with size 1502.4 KiB\n",
      "23/09/19 09:59:06 WARN DAGScheduler: Broadcasting large task binary with size 1503.0 KiB\n",
      "23/09/19 09:59:06 WARN DAGScheduler: Broadcasting large task binary with size 1504.1 KiB\n",
      "23/09/19 09:59:06 WARN DAGScheduler: Broadcasting large task binary with size 1505.3 KiB\n",
      "23/09/19 09:59:06 WARN DAGScheduler: Broadcasting large task binary with size 1506.7 KiB\n",
      "23/09/19 09:59:07 WARN DAGScheduler: Broadcasting large task binary with size 1507.1 KiB\n",
      "23/09/19 09:59:07 WARN DAGScheduler: Broadcasting large task binary with size 1507.7 KiB\n",
      "23/09/19 09:59:07 WARN DAGScheduler: Broadcasting large task binary with size 1508.6 KiB\n",
      "23/09/19 09:59:08 WARN DAGScheduler: Broadcasting large task binary with size 1509.8 KiB\n",
      "23/09/19 09:59:08 WARN DAGScheduler: Broadcasting large task binary with size 1511.2 KiB\n",
      "23/09/19 09:59:08 WARN DAGScheduler: Broadcasting large task binary with size 1511.7 KiB\n",
      "23/09/19 09:59:08 WARN DAGScheduler: Broadcasting large task binary with size 1512.2 KiB\n",
      "23/09/19 09:59:09 WARN DAGScheduler: Broadcasting large task binary with size 1513.1 KiB\n",
      "23/09/19 09:59:09 WARN DAGScheduler: Broadcasting large task binary with size 1514.5 KiB\n",
      "23/09/19 09:59:09 WARN DAGScheduler: Broadcasting large task binary with size 1516.1 KiB\n",
      "23/09/19 09:59:10 WARN DAGScheduler: Broadcasting large task binary with size 1516.5 KiB\n",
      "23/09/19 09:59:10 WARN DAGScheduler: Broadcasting large task binary with size 1517.1 KiB\n",
      "23/09/19 09:59:10 WARN DAGScheduler: Broadcasting large task binary with size 1518.0 KiB\n",
      "23/09/19 09:59:10 WARN DAGScheduler: Broadcasting large task binary with size 1519.2 KiB\n",
      "23/09/19 09:59:11 WARN DAGScheduler: Broadcasting large task binary with size 1520.8 KiB\n",
      "23/09/19 09:59:11 WARN DAGScheduler: Broadcasting large task binary with size 1521.2 KiB\n",
      "23/09/19 09:59:11 WARN DAGScheduler: Broadcasting large task binary with size 1521.8 KiB\n",
      "23/09/19 09:59:11 WARN DAGScheduler: Broadcasting large task binary with size 1522.9 KiB\n",
      "23/09/19 09:59:12 WARN DAGScheduler: Broadcasting large task binary with size 1524.6 KiB\n",
      "23/09/19 09:59:12 WARN DAGScheduler: Broadcasting large task binary with size 1526.2 KiB\n",
      "23/09/19 09:59:12 WARN DAGScheduler: Broadcasting large task binary with size 1526.7 KiB\n",
      "23/09/19 09:59:13 WARN DAGScheduler: Broadcasting large task binary with size 1527.3 KiB\n",
      "23/09/19 09:59:13 WARN DAGScheduler: Broadcasting large task binary with size 1528.2 KiB\n",
      "23/09/19 09:59:13 WARN DAGScheduler: Broadcasting large task binary with size 1529.0 KiB\n",
      "23/09/19 09:59:13 WARN DAGScheduler: Broadcasting large task binary with size 1530.3 KiB\n",
      "23/09/19 09:59:14 WARN DAGScheduler: Broadcasting large task binary with size 1530.8 KiB\n",
      "23/09/19 09:59:14 WARN DAGScheduler: Broadcasting large task binary with size 1531.3 KiB\n",
      "23/09/19 09:59:14 WARN DAGScheduler: Broadcasting large task binary with size 1532.2 KiB\n",
      "23/09/19 09:59:14 WARN DAGScheduler: Broadcasting large task binary with size 1533.4 KiB\n",
      "23/09/19 09:59:15 WARN DAGScheduler: Broadcasting large task binary with size 1534.8 KiB\n",
      "23/09/19 09:59:15 WARN DAGScheduler: Broadcasting large task binary with size 1535.3 KiB\n",
      "23/09/19 09:59:15 WARN DAGScheduler: Broadcasting large task binary with size 1535.9 KiB\n",
      "23/09/19 09:59:16 WARN DAGScheduler: Broadcasting large task binary with size 1537.0 KiB\n",
      "23/09/19 09:59:16 WARN DAGScheduler: Broadcasting large task binary with size 1538.9 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/19 09:59:17 WARN DAGScheduler: Broadcasting large task binary with size 1539.8 KiB\n",
      "23/09/19 09:59:17 WARN DAGScheduler: Broadcasting large task binary with size 1540.3 KiB\n",
      "23/09/19 09:59:17 WARN DAGScheduler: Broadcasting large task binary with size 1540.9 KiB\n",
      "23/09/19 09:59:17 WARN DAGScheduler: Broadcasting large task binary with size 1541.8 KiB\n",
      "23/09/19 09:59:18 WARN DAGScheduler: Broadcasting large task binary with size 1542.9 KiB\n",
      "23/09/19 09:59:18 WARN DAGScheduler: Broadcasting large task binary with size 1544.5 KiB\n",
      "23/09/19 09:59:18 WARN DAGScheduler: Broadcasting large task binary with size 1545.0 KiB\n",
      "23/09/19 09:59:18 WARN DAGScheduler: Broadcasting large task binary with size 1545.6 KiB\n",
      "23/09/19 09:59:19 WARN DAGScheduler: Broadcasting large task binary with size 1546.5 KiB\n",
      "23/09/19 09:59:19 WARN DAGScheduler: Broadcasting large task binary with size 1547.6 KiB\n",
      "23/09/19 09:59:19 WARN DAGScheduler: Broadcasting large task binary with size 1230.5 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(13424,[0,1,2,4,3...|  1.0|[-0.2826313667944...|[0.36233064133634...|       1.0|\n",
      "|(13424,[0,1,2,12,...|  1.0|[-1.0362358508548...|[0.11180135848404...|       1.0|\n",
      "|(13424,[0,1,2,13,...|  0.0|[1.06730480541037...|[0.89422182052853...|       0.0|\n",
      "|(13424,[0,1,2,20,...|  1.0|[-1.3671619444239...|[0.06097810741251...|       1.0|\n",
      "|(13424,[0,1,2,41,...|  0.0|[-0.0131523174510...|[0.49342422043712...|       1.0|\n",
      "|(13424,[0,1,2,91,...|  1.0|[-0.8181686294204...|[0.16296406874331...|       1.0|\n",
      "|(13424,[0,1,3,9,1...|  0.0|[1.49047297190227...|[0.95170586715115...|       0.0|\n",
      "|(13424,[0,1,4,137...|  1.0|[-1.0677598556183...|[0.10569212472182...|       1.0|\n",
      "|(13424,[0,1,5,15,...|  0.0|[0.90289253309279...|[0.85885168938253...|       0.0|\n",
      "|(13424,[0,1,9,14,...|  0.0|[0.18952079632782...|[0.59364192595540...|       0.0|\n",
      "|(13424,[0,1,10,20...|  1.0|[-0.9197227001574...|[0.13711689730575...|       1.0|\n",
      "|(13424,[0,1,11,32...|  0.0|[1.18045922248194...|[0.91379817989410...|       0.0|\n",
      "|(13424,[0,1,13,18...|  1.0|[-1.3794088435294...|[0.05959058777171...|       1.0|\n",
      "|(13424,[0,1,13,27...|  1.0|[-1.3491532785289...|[0.06307335623543...|       1.0|\n",
      "|(13424,[0,1,14,18...|  0.0|[0.61282383798494...|[0.77305590806898...|       0.0|\n",
      "|(13424,[0,1,21,27...|  0.0|[0.99798118474947...|[0.88037250045630...|       0.0|\n",
      "|(13424,[0,1,23,63...|  0.0|[-1.0120010839284...|[0.11670579190199...|       1.0|\n",
      "|(13424,[0,1,27,37...|  1.0|[-1.1668130848118...|[0.08837608178772...|       1.0|\n",
      "|(13424,[0,1,27,85...|  0.0|[0.39494157046384...|[0.68780623812322...|       0.0|\n",
      "|(13424,[0,1,31,43...|  0.0|[1.49047297190227...|[0.95170586715115...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the GBTClassifier class from pyspark.ml.classification\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "# Create a GBTClassifier model instance\n",
    "model_3 = GBTClassifier()\n",
    "\n",
    "# Split the 'final_data' DataFrame into training and testing sets with a 70-30 ratio\n",
    "train, test = final_data.randomSplit([0.7,0.3])\n",
    "\n",
    "# Fit the GBTClassifier model on the training data\n",
    "model_fit_3 = model_3.fit(train)\n",
    "\n",
    "# Transform the test data using the fitted model to make predictions\n",
    "model_test_3 = model_fit_3.transform(test)\n",
    "\n",
    "# Show the results, including predictions, in the 'model_test_3' DataFrame\n",
    "model_test_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3b8eba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/19 09:59:20 WARN DAGScheduler: Broadcasting large task binary with size 1235.1 KiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9447222142282418"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the MulticlassClassificationEvaluator class from pyspark.ml.evaluation\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Create an instance of the MulticlassClassificationEvaluator\n",
    "eval_model_3 = MulticlassClassificationEvaluator()\n",
    "\n",
    "# Evaluate the performance of the model_3 (Gradient-Boosted Tree) on the test data and calculate the accuracy\n",
    "acc_3 = eval_model_3.evaluate(model_test_3)\n",
    "\n",
    "\n",
    "# Display the accuracy of the model\n",
    "acc_3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
