{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01880f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Customer Churn\n",
    "\n",
    "# A marketing agency has many customers that use their service to produce ads for the\n",
    "# client/customer websites. They've noticed that they have quite a bit of churn in clients.\n",
    "# They basically randomly assign account managers right now, but want you to create a machine \n",
    "# learning model that will help predict which customers will churn (stop buying their service)\n",
    "# so that they can correctly assign the customers most at risk to churn an account manager. \n",
    "# We will create a classification algorithm that will help classify whether or not a customer \n",
    "# churned. Then the company can test this against incoming data for future customers to predict which customers will churn \n",
    "# and assign them an account manager.\n",
    "\n",
    "# The data is saved as customer_churn.csv. Here are the fields and their definitions:\n",
    "\n",
    "# Name : Name of the latest contact at Company\n",
    "# Age: Customer Age\n",
    "# Total_Purchase: Total Ads Purchased\n",
    "# Account_Manager: Binary 0=No manager, 1= Account manager assigned\n",
    "# Years: Totaly Years as a customer\n",
    "# Num_sites: Number of websites that use the service.\n",
    "# Onboard_date: Date that the name of the latest contact was onboarded\n",
    "# Location: Client HQ Address\n",
    "# Company: Name of Client Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca3a61a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/13 16:34:55 WARN Utils: Your hostname, mina-VirtualBox resolves to a loopback address: 127.0.1.1; using 192.168.1.143 instead (on interface enp0s3)\n",
      "23/09/13 16:34:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/13 16:34:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/09/13 16:34:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Import the findspark module and initialize it with the specified Spark path\n",
    "import findspark\n",
    "findspark.init('/home/mina/python-spark/spark-3.4.0-bin-hadoop3/')\n",
    "\n",
    "# Import the pyspark module and the SparkSession class\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session with the specified app name\n",
    "spark = SparkSession.builder.appName('Custmoer_Churn').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4201f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a CSV file named 'customer_churn.csv' into a DataFrame\n",
    "# The 'inferSchema=True' option infers data types for columns, and 'header=True' treats the first row as column names\n",
    "dataset = spark.read.csv('customer_churn.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147f4f02",
   "metadata": {},
   "source": [
    "<h1>Check out the data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce64eeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Names='Cameron Williams', Age=42.0, Total_Purchase=11066.8, Account_Manager=0, Years=7.22, Num_Sites=8.0, Onboard_date=datetime.datetime(2013, 8, 30, 7, 0, 40), Location='10265 Elizabeth Mission Barkerburgh, AK 89518', Company='Harvey LLC', Churn=1) \n",
      "\n",
      "Row(Names='Kevin Mueller', Age=41.0, Total_Purchase=11916.22, Account_Manager=0, Years=6.5, Num_Sites=11.0, Onboard_date=datetime.datetime(2013, 8, 13, 0, 38, 46), Location='6157 Frank Gardens Suite 019 Carloshaven, RI 17756', Company='Wilson PLC', Churn=1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through the first two rows of the dataset and print each row\n",
    "for record in dataset.head(2):\n",
    "    print(record, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb28a7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of the dataset\n",
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99b37fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+--------------------+--------------------+-------------------+\n",
      "|summary|        Names|              Age|   Total_Purchase|   Account_Manager|            Years|         Num_Sites|            Location|             Company|              Churn|\n",
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+--------------------+--------------------+-------------------+\n",
      "|  count|          900|              900|              900|               900|              900|               900|                 900|                 900|                900|\n",
      "|   mean|         null|41.81666666666667|10062.82403333334|0.4811111111111111| 5.27315555555555| 8.587777777777777|                null|                null|0.16666666666666666|\n",
      "| stddev|         null|6.127560416916251|2408.644531858096|0.4999208935073339|1.274449013194616|1.7648355920350969|                null|                null| 0.3728852122772358|\n",
      "|    min|   Aaron King|             22.0|            100.0|                 0|              1.0|               3.0|00103 Jeffrey Cre...|     Abbott-Thompson|                  0|\n",
      "|    25%|         null|             38.0|          8480.93|                 0|             4.45|               7.0|                null|                null|                  0|\n",
      "|    50%|         null|             42.0|         10041.13|                 0|             5.21|               8.0|                null|                null|                  0|\n",
      "|    75%|         null|             46.0|         11758.69|                 1|             6.11|              10.0|                null|                null|                  0|\n",
      "|    max|Zachary Walsh|             65.0|         18026.01|                 1|             9.15|              14.0|Unit 9800 Box 287...|Zuniga, Clark and...|                  1|\n",
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute summary statistics and display them\n",
    "dataset.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ec3f4b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Names',\n",
       " 'Age',\n",
       " 'Total_Purchase',\n",
       " 'Account_Manager',\n",
       " 'Years',\n",
       " 'Num_Sites',\n",
       " 'Onboard_date',\n",
       " 'Location',\n",
       " 'Company',\n",
       " 'Churn']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the names of columns\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a54ffe2",
   "metadata": {},
   "source": [
    "<h1>Format for MLlib</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03502657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for creating VectorAssembly\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create a VectorAssembler to assemble selected columns into a feature vector\n",
    "assembler = VectorAssembler(inputCols=['Age', 'Total_Purchase',\n",
    "                                       'Account_Manager' ,'Years',\n",
    "                                       'Num_Sites'],\n",
    "                                        outputCol='Features')\n",
    "\n",
    "# Transform the DataFrame using the VectorAssembler to add the 'Features' column\n",
    "output = assembler.transform(dataset)\n",
    "\n",
    "# Select the desired columns 'Features' and 'Churn' from the transformed DataFrame\n",
    "final_dataset = output.select('Features','Churn')\n",
    "\n",
    "# Split the final dataset into training and testing sets\n",
    "train_churn,test_churn = final_dataset.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ee5f51",
   "metadata": {},
   "source": [
    "<h1>Fit the Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43d40999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            Features|Churn|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[22.0,11254.38,1....|  0.0|[4.63431745785773...|[0.99038069561911...|       0.0|\n",
      "|[25.0,9672.03,0.0...|  0.0|[4.49286992774660...|[0.98893530961276...|       0.0|\n",
      "|[26.0,8787.39,1.0...|  1.0|[0.71154311629332...|[0.67074204239835...|       0.0|\n",
      "|[26.0,8939.61,0.0...|  0.0|[6.11524442531096...|[0.99779593285916...|       0.0|\n",
      "|[28.0,11128.95,1....|  0.0|[4.16352113113749...|[0.98468548374846...|       0.0|\n",
      "|[28.0,11204.23,0....|  0.0|[1.67555171650752...|[0.84231460619983...|       0.0|\n",
      "|[28.0,11245.38,0....|  0.0|[3.60424545734254...|[0.97351269896182...|       0.0|\n",
      "|[29.0,9617.59,0.0...|  0.0|[4.23322410369632...|[0.98570185474190...|       0.0|\n",
      "|[29.0,10203.18,1....|  0.0|[3.76817665844374...|[0.97742716661120...|       0.0|\n",
      "|[29.0,12711.15,0....|  0.0|[5.16475795041401...|[0.99431802349618...|       0.0|\n",
      "|[29.0,13240.01,1....|  0.0|[6.64336621266846...|[0.99869905974111...|       0.0|\n",
      "|[29.0,13255.05,1....|  0.0|[4.14239275069374...|[0.98436358367369...|       0.0|\n",
      "|[30.0,8403.78,1.0...|  0.0|[5.81138834903605...|[0.99701566145482...|       0.0|\n",
      "|[30.0,8677.28,1.0...|  0.0|[4.15474151185629...|[0.98455252230517...|       0.0|\n",
      "|[30.0,10183.98,1....|  0.0|[2.88571145034959...|[0.94713556609535...|       0.0|\n",
      "|[30.0,10744.14,1....|  1.0|[1.81976964678590...|[0.86053848418398...|       0.0|\n",
      "|[30.0,11575.37,1....|  1.0|[3.96522802595763...|[0.98138921761459...|       0.0|\n",
      "|[31.0,5387.75,0.0...|  0.0|[2.38740981911393...|[0.91586218715897...|       0.0|\n",
      "|[31.0,7073.61,0.0...|  0.0|[2.89777219769326...|[0.94773619876218...|       0.0|\n",
      "|[31.0,8829.83,1.0...|  0.0|[4.35906069434933...|[0.98737113212227...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a LogisticRegression model\n",
    "lr_churn = LogisticRegression(featuresCol='Features',labelCol='Churn')\n",
    "\n",
    "# Fit the model on the training data\n",
    "fit_model = lr_churn.fit(train_churn)\n",
    "\n",
    "# Get the summary of the model's training\n",
    "training_sum = fit_model.summary\n",
    "\n",
    "# Show columns from the predictions DataFrame\n",
    "training_sum.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a4148e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+\n",
      "|summary|              Churn|         prediction|\n",
      "+-------+-------------------+-------------------+\n",
      "|  count|                615|                615|\n",
      "|   mean|0.17560975609756097|0.12682926829268293|\n",
      "| stddev| 0.3807975223122206|0.33305250212923654|\n",
      "|    min|                0.0|                0.0|\n",
      "|    max|                1.0|                1.0|\n",
      "+-------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_sum.predictions.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fbbae30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            Features|Churn|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[27.0,8628.8,1.0,...|    0|[5.39365940844732...|[0.99547525663991...|       0.0|\n",
      "|[28.0,8670.98,0.0...|    0|[7.44872594739648...|[0.99941815586819...|       0.0|\n",
      "|[28.0,9090.43,1.0...|    0|[1.57429680591052...|[0.82839528947316...|       0.0|\n",
      "|[29.0,5900.78,1.0...|    0|[4.05432909154966...|[0.98294867665427...|       0.0|\n",
      "|[29.0,8688.17,1.0...|    1|[2.71373002577927...|[0.93783197639881...|       0.0|\n",
      "|[29.0,9378.24,0.0...|    0|[4.53169247112310...|[0.98935215136931...|       0.0|\n",
      "|[29.0,11274.46,1....|    0|[4.45037849011203...|[0.98846056547699...|       0.0|\n",
      "|[30.0,6744.87,0.0...|    0|[3.28031662080738...|[0.96374734747347...|       0.0|\n",
      "|[30.0,7960.64,1.0...|    1|[3.04744413501567...|[0.95467205418320...|       0.0|\n",
      "|[30.0,8874.83,0.0...|    0|[2.98778864425551...|[0.95201939972887...|       0.0|\n",
      "|[30.0,10960.52,1....|    0|[2.43366648244180...|[0.91935877754148...|       0.0|\n",
      "|[30.0,12788.37,0....|    0|[2.32745939681652...|[0.91112582475873...|       0.0|\n",
      "|[30.0,13473.35,0....|    0|[2.54676774020389...|[0.92735606822215...|       0.0|\n",
      "|[31.0,5304.6,0.0,...|    0|[3.18794945301969...|[0.96037826695777...|       0.0|\n",
      "|[31.0,8688.21,0.0...|    0|[6.29435562310363...|[0.99815670513076...|       0.0|\n",
      "|[31.0,9574.89,0.0...|    0|[3.14577616138663...|[0.95874196781380...|       0.0|\n",
      "|[32.0,6367.22,1.0...|    0|[2.91061671135181...|[0.94836877043152...|       0.0|\n",
      "|[32.0,7896.65,0.0...|    0|[3.25342211205000...|[0.96279588691936...|       0.0|\n",
      "|[32.0,8617.98,1.0...|    1|[1.09112172485171...|[0.74859289085344...|       0.0|\n",
      "|[32.0,13630.93,0....|    0|[2.13075580224698...|[0.89385673782527...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "pre_and_label = fit_model.evaluate(test_churn)\n",
    "\n",
    "# Show the predictions made on the test dataset\n",
    "pre_and_label.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "31489a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+\n",
      "|summary|              Churn|         prediction|\n",
      "+-------+-------------------+-------------------+\n",
      "|  count|                285|                285|\n",
      "|   mean|0.14736842105263157|0.12982456140350876|\n",
      "| stddev|0.35509632850873796| 0.3367015397342311|\n",
      "|    min|                  0|                0.0|\n",
      "|    max|                  1|                1.0|\n",
      "+-------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre_and_label.predictions.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb75da80",
   "metadata": {},
   "source": [
    "<h1>Evaluate Results</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44259889",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8148148148148149"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Evaluation module\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Create a BinaryClassificationEvaluator specifying the raw prediction and label columns\n",
    "churn_evl = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='Churn')\n",
    "\n",
    "# Calculate the AUC by evaluating the predictions\n",
    "AUC = churn_evl.evaluate(pre_and_label.predictions)\n",
    "\n",
    "# Display the calculated AUC\n",
    "AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813b6e55",
   "metadata": {},
   "source": [
    "<h1>Predict on brand new unlabeled data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb7bf648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and fit a logistic regression model on the final dataset\n",
    "final_lr_model = lr_churn.fit(final_dataset)\n",
    "\n",
    "# Read data from 'new_customers.csv' into a DataFrame\n",
    "new_customers = spark.read.csv('new_customers.csv',inferSchema=True,\n",
    "                              header=True)\n",
    "\n",
    "# Print the schema of the 'new_customers' DataFrame\n",
    "new_customers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "189deae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform the 'new_customers' DataFrame using the VectorAssembler\n",
    "test_new_customers = assembler.transform(new_customers)\n",
    "\n",
    "# Print the schema of the 'test_new_customers' DataFrame\n",
    "test_new_customers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9285568b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+\n",
      "|         Company|prediction|\n",
      "+----------------+----------+\n",
      "|        King Ltd|       0.0|\n",
      "|   Cannon-Benson|       1.0|\n",
      "|Barron-Robertson|       1.0|\n",
      "|   Sexton-Golden|       1.0|\n",
      "|        Wood LLC|       0.0|\n",
      "|   Parks-Robbins|       1.0|\n",
      "+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the trained logistic regression model to make predictions on 'test_new_customers'\n",
    "final_results = final_lr_model.transform(test_new_customers)\n",
    "\n",
    "# Select the 'Company' and 'prediction' columns from the results\n",
    "final_results.select('Company','prediction').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
