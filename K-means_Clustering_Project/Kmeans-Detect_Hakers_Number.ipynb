{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16650409",
   "metadata": {},
   "source": [
    "<h1>Project_Desciption</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e154eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A large technology have been hacked! Luckily their engineers have grabbed valuable data about the hacks, including \n",
    "# information like session time,locations, wpm typing speed, etc. The engineer relates to you what she has\n",
    "# been able to figure out so far, she has been able to grab meta data of each session that the hackers used to \n",
    "# connect to their servers. These are the features of the data:\n",
    "\n",
    "#     'Session_Connection_Time': How long the session lasted in minutes\n",
    "#     'Bytes Transferred': Number of MB transferred during session\n",
    "#     'Kali_Trace_Used': Indicates if the hacker was using Kali Linux\n",
    "#     'Servers_Corrupted': Number of server corrupted during the attack\n",
    "#     'Pages_Corrupted': Number of pages illegally accessed\n",
    "#     'Location': Location attack came from (Probably useless because the hackers used VPNs)\n",
    "#     'WPM_Typing_Speed': Their estimated typing speed based on session logs.\n",
    "        \n",
    "# The technology firm has 3 potential hackers that perpetrated the attack. Their certain of the first two hackers but\n",
    "# they aren't very sure if the third hacker was involved or not. We want to figure out whether or not the third \n",
    "# suspect had anything to do with the attacks, or was it just two hackers?\n",
    "\n",
    "# One last key fact, the forensic engineer knows that the hackers trade off attacks. Meaning they should each have \n",
    "# roughly the same amount of attacks. For example if there were 100 total attacks, then in a 2 hacker situation each\n",
    "# should have about 50 hacks, in a three hacker situation each would have about 33 hacks. The engineer believes this \n",
    "# is the key element to solving this, but doesn't know how to distinguish this unlabeled data into groups of hackers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea280ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/13 18:37:26 WARN Utils: Your hostname, mina-VirtualBox resolves to a loopback address: 127.0.1.1; using 192.168.1.143 instead (on interface enp0s3)\n",
      "23/09/13 18:37:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/13 18:37:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Import the findspark module and initialize it with the specified Spark path\n",
    "import findspark\n",
    "findspark.init('/home/mina/python-spark/spark-3.4.0-bin-hadoop3/')\n",
    "\n",
    "# Import the pyspark module and the SparkSession class\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session with the application name 'hack_data'.\n",
    "spark = SparkSession.builder.appName('hack_data').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80e6c1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Session_Connection_Time: double (nullable = true)\n",
      " |-- Bytes Transferred: double (nullable = true)\n",
      " |-- Kali_Trace_Used: integer (nullable = true)\n",
      " |-- Servers_Corrupted: double (nullable = true)\n",
      " |-- Pages_Corrupted: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- WPM_Typing_Speed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read a CSV file named 'hack_data.csv' into a DataFrame\n",
    "# The 'inferSchema=True' option infers data types for columns, and 'header=True' treats the first row as column names\n",
    "dataset = spark.read.csv('hack_data.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Print the schema of the dataset\n",
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "269f8606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/13 18:37:39 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
      "|summary|Session_Connection_Time| Bytes Transferred|   Kali_Trace_Used|Servers_Corrupted|   Pages_Corrupted|   Location|  WPM_Typing_Speed|\n",
      "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
      "|  count|                    334|               334|               334|              334|               334|        334|               334|\n",
      "|   mean|     30.008982035928145| 607.2452694610777|0.5119760479041916|5.258502994011977|10.838323353293413|       null|57.342395209580864|\n",
      "| stddev|     14.088200614636158|286.33593163576757|0.5006065264451406| 2.30190693339697|  3.06352633036022|       null| 13.41106336843464|\n",
      "|    min|                    1.0|              10.0|                 0|              1.0|               6.0|Afghanistan|              40.0|\n",
      "|    25%|                   18.0|            372.05|                 0|             3.12|               8.0|       null|             44.12|\n",
      "|    50%|                   31.0|            600.84|                 1|             5.25|               9.0|       null|              48.6|\n",
      "|    75%|                   42.0|            844.01|                 1|              7.4|              14.0|       null|             70.58|\n",
      "|    max|                   60.0|            1330.5|                 1|             10.0|              15.0|   Zimbabwe|              75.0|\n",
      "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Generate summary statistics for the columns in the 'dataset' DataFrame.\n",
    "# 'dataset.summary()' computes common summary statistics like count, mean, stddev, min, max, etc.\n",
    "dataset.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fec8b39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Session_Connection_Time=8.0, Bytes Transferred=391.09, Kali_Trace_Used=1, Servers_Corrupted=2.96, Pages_Corrupted=7.0, Location='Slovenia', WPM_Typing_Speed=72.37)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the first row of the 'dataset' DataFrame\n",
    "dataset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ef91f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Session_Connection_Time',\n",
       " 'Bytes Transferred',\n",
       " 'Kali_Trace_Used',\n",
       " 'Servers_Corrupted',\n",
       " 'Pages_Corrupted',\n",
       " 'Location',\n",
       " 'WPM_Typing_Speed']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the list of column names present in the 'dataset' DataFrame.\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a75485fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of column names that represent features of interest.\n",
    "feature_cols = ['Session_Connection_Time',\n",
    " 'Bytes Transferred',\n",
    " 'Kali_Trace_Used',\n",
    " 'Servers_Corrupted',\n",
    " 'Pages_Corrupted',\n",
    " 'WPM_Typing_Speed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfa9aafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Session_Connection_Time: double (nullable = true)\n",
      " |-- Bytes Transferred: double (nullable = true)\n",
      " |-- Kali_Trace_Used: integer (nullable = true)\n",
      " |-- Servers_Corrupted: double (nullable = true)\n",
      " |-- Pages_Corrupted: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- WPM_Typing_Speed: double (nullable = true)\n",
      " |-- Features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules for creating VectorAssembly\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create a VectorAssembler to assemble selected columns into a feature vector\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='Features')\n",
    "\n",
    "# Transform the DataFrame using the VectorAssembler to add the 'Features' column\n",
    "output_dataset = assembler.transform(dataset)\n",
    "\n",
    "# Print the schema of the dataset\n",
    "output_dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "409f562e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Session_Connection_Time=8.0, Bytes Transferred=391.09, Kali_Trace_Used=1, Servers_Corrupted=2.96, Pages_Corrupted=7.0, Location='Slovenia', WPM_Typing_Speed=72.37, Features=DenseVector([8.0, 391.09, 1.0, 2.96, 7.0, 72.37]), Features_scaled=DenseVector([0.5679, 1.3658, 1.9976, 1.2859, 2.2849, 5.3963]))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the StandardScaler class from the PySpark MLlib library\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Create an instance of the StandardScaler class\n",
    "# - 'inputCol' specifies the input column to scale ('Features' in this case)\n",
    "# - 'outputCol' specifies the name of the output scaled column ('Features_scaled' in this case)\n",
    "scaler = StandardScaler(inputCol='Features', \n",
    "                        outputCol='Features_scaled')\n",
    "\n",
    "# Fit the scaler to the 'output_dataset'\n",
    "# This computes summary statistics and prepares the scaler for transformation\n",
    "scaler_fit_db = scaler.fit(output_dataset)\n",
    "\n",
    "# Transform the 'output_dataset' using the fitted scaler\n",
    "# This scales the 'features' column and adds a new column 'features_scaled'\n",
    "scaler_trans_db = scaler_fit_db.transform(output_dataset)\n",
    "scaler_trans_db.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886c9b81",
   "metadata": {},
   "source": [
    "<h2>Time to find out whether its 2 or 3!</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0398703b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434.1492898715821"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the KMeans class from the PySpark MLlib library\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Create a KMeans model with 'Features_scaled' as the feature column and 'k' clusters set to 2\n",
    "kmeans1 = KMeans(featuresCol='Features_scaled' , k = 3 )\n",
    "\n",
    "# Create another KMeans model with 'Features_scaled' as the feature column and 'k' clusters set to 3\n",
    "kmeans2 = KMeans(featuresCol='Features_scaled' , k = 2 )\n",
    "\n",
    "# Fit the first KMeans model (kmeans1) to the 'scaler_trans_db' dataset\n",
    "model_1 = kmeans1.fit(scaler_trans_db)\n",
    "\n",
    "# Fit the second KMeans model (kmeans2) to the 'scaler_trans_db' dataset\n",
    "model_2 = kmeans2.fit(scaler_trans_db)\n",
    "\n",
    "# Get the training cost (inertia) for the first KMeans model.\n",
    "# The training cost is a measure of the sum of squared distances from each point to its assigned cluster center\n",
    "model_1.summary.trainingCost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25a8ff86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "601.7707512676691"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the training cost (inertia) for the second K-Means model (model_2)\n",
    "# The training cost is a measure of the sum of squared distances from each point to its assigned cluster center\n",
    "training_cost_2 = model_2.summary.trainingCost\n",
    "model_2.summary.trainingCost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe540b0",
   "metadata": {},
   "source": [
    "<h3>Not much to be gained from trainingCosts , after all, we would expect that as K increases, the trainingCost decreases. We could however continue the analysis by seeing the drop from K=3 to K=4 to check if the clustering favors even or odd numbers. This won't be substantial, but its worth a look:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c90bcafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With K=2\n",
      "Within Set Sum of Squared Errors = 601.7707512676691\n",
      "------------------------------------------------------------\n",
      "With K=3\n",
      "Within Set Sum of Squared Errors = 434.1492898715821\n",
      "------------------------------------------------------------\n",
      "With K=4\n",
      "Within Set Sum of Squared Errors = 267.1336116887894\n",
      "------------------------------------------------------------\n",
      "With K=5\n",
      "Within Set Sum of Squared Errors = 245.4269716116671\n",
      "------------------------------------------------------------\n",
      "With K=6\n",
      "Within Set Sum of Squared Errors = 231.65828275358692\n",
      "------------------------------------------------------------\n",
      "With K=7\n",
      "Within Set Sum of Squared Errors = 210.1500778742552\n",
      "------------------------------------------------------------\n",
      "With K=8\n",
      "Within Set Sum of Squared Errors = 203.23698492146255\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for k in range(2,9):\n",
    "    kmeans = KMeans(featuresCol='Features_scaled',k=k)\n",
    "    model = kmeans.fit(scaler_trans_db)\n",
    "    trainingCost = model.summary.trainingCost\n",
    "    print(\"With K={}\".format(k))\n",
    "    print(\"Within Set Sum of Squared Errors = \" + str(trainingCost))\n",
    "    print('--'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eccde52",
   "metadata": {},
   "source": [
    "<h3>Nothing definitive can be said with the above, but the last key fact that the engineer mentioned was that the attacks should be evenly numbered between the hackers! Let's check with the transform and prediction columns that result form this!</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c892766f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|   83|\n",
      "|         2|   84|\n",
      "|         0|  167|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the first K-Means model (model_1) to make predictions on the 'scaler_trans_db' dataset\n",
    "# The resulting DataFrame contains a 'prediction' column indicating the cluster assignment\n",
    "pre_column = model_1.transform(scaler_trans_db).select('prediction')\n",
    "\n",
    "# Group the data by the 'prediction' column and count the number of data points in each cluster\n",
    "pre_column.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac649d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|  167|\n",
      "|         0|  167|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the second K-Means model (model_2) to make predictions on the 'scaler_trans_db' dataset\n",
    "# The resulting DataFrame contains a 'prediction' column indicating the cluster assignment\n",
    "pre_column = model_2.transform(scaler_trans_db).select('prediction')\n",
    "\n",
    "# Group the data by the 'prediction' column and count the number of data points in each cluster\n",
    "pre_column.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467bd791",
   "metadata": {},
   "source": [
    "<h2>Finally it has been solved! It was 2 hackers. In fact, our clustering algorithm created two equally sized clusters with K=2, no way that is a coincidence!</h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
